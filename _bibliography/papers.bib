---
---


@inproceedings{Wu:2020efficient,
  title = {Lite Transformer with Long-Short Range Attention},
  author = {Wu*, Zhanghao and Liu*, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
  booktitle = {ICLR},
  year = {2020},
  url = {https://openreview.net/forum?id=ByeMPlHKPH},
  pdf = {https://openreview.net/pdf?id=ByeMPlHKPH},
  slides = {Presentation_ENMT.pdf},
  image = {enmt.png},
  website = {https://zhanghaowu.me/pubs/mbt/index.html},
  abstract = {Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications since mobile phones are tightly constrained by the hardware resources and battery. In this paper, we investigate the mobile setting (under 500M Mult-Adds) for NLP tasks to facilitate the deployment on the edge devices. We present Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group captures the long-distance relationship (by attention). Based on this primitive, we design Lite Transformer that is tailored for the mobile NLP application. Our Lite Transformer demonstrates consistent improvement over the transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. It outperforms the transformer on WMTâ€™14 English-French by 1.2 BLEU under 500M Mult-Adds and 1.7 BLEU under 100M Mult-Adds, and reduces the computation of transformer base model by 2.5x. Further, with general techniques, our Lite Transformer achieves 18.2x model size compression. For language modeling, our Lite Transformer also achieves 3.8 lower perplexity than the transformer around 500M Mult-Adds. Without the costly architecture search that requires more than 250 GPU years, our Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU under the mobile setting.}
}

@article{Wu:2020hat,
  title = {Efficient Hardware-Aware Transformers},
  author = {Wang*, Hanrui and Wu*, Zhanghao and Liu*, Zhijian and Cai, Han and Zhu, Ligeng and Han, Song},
  journal = {Submitted to ACL},
  year = {2020},
  image = {hat.png},
  abstract = {(The name of the title is changed due to the anonymity of ACL.) Remarkably, when running WMT'14 translation task on Raspberry Pi 4 ARM CPU, HAT can achieve 3x speedup, 3.7x smaller size over baseline Transformer; 2.7x speedup, 3.6x smaller size over Evolved Transformer with 12,041x less search cost and no performance loss.}
}

@article{Wu:2020FaceMix,
    title = {FaceMix: Privacy-preserving Facial Attribute Classification on the Cloud},
    author = {Liu*, Zhijian and Wu*, Zhanghao and Zhu, Ligeng and Gan, Chuang and Han, Song},
    journal = {Submitted to ECCV},
    year = {2020},
    slides = {Privacy-Preserving_Edge-Cloud_Inference.pdf},
    demo = {https://www.youtube.com/watch?v=nLVWIl-h5Zc&feature=youtu.be},
    image = {privacy_cvpr.png},
    abstract = {Deep neural networks are widely deployed on edge devices (e.g., for facial attribute classification). Users either perform the inference locally (i.e., edge-based) or send the data to the cloud and run the inference remotely (i.e., cloud-based). However, both solutions have their limitations: edge devices are heavily constrained by insufficient hardware resources and cannot afford to run large models; cloud servers, if not trustworthy, will raise serious privacy issues. In this paper, we mediate between the resource-constrained edge devices and the privacy-invasive cloud servers by introducing a novel privacy-preserving edge-cloud inference framework, FaceMix, for the facial attribute classification networks. We offload the majority of the computations to the cloud and leverage a pair of encryption and decryption functions to protect the privacy of the data transmitted to the cloud. Our framework has three advantages. First, it is privacy-preserving as our encryption cannot be inverted without user's private key. Second, our framework is accuracy-preserving because our encryption takes advantage of the linearity, and we train the model in an encryption-aware manner to help maintain the accuracy. Third, our solution is efficient on the edge since the majority of the workload is delegated to the cloud, and our encryption and decryption processes introduce very few extra computations. Also, our framework introduces small communication overhead and maintains high hardware utilization on the cloud. Extensive experiments on multiple facial attribute classification datasets demonstrate that our framework can greatly reduce the local computations on the edge (to fewer than 20% of FLOPs) with negligible loss of accuracy and no leakages of private information.}
}

@inproceedings{Wu:2019vaeda, 
  title = {Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification}, 
  author = {Wu, Zhanghao and Wang, Shuai and Qian, Yanmin and Yu, Kai}, 
  booktitle = {Interspeech}, 
  doi = {10.21437/Interspeech.2019-2248},
  year = {2019},
  url = {http://dx.doi.org/10.21437/Interspeech.2019-2248},
  pdf = {https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2248.pdf},
  oral = {},
  slides = {Presentations_VAE_DA.pdf},
  image = {vae_da.png},
  abstract = {Domain or environment mismatch between training and testing, such as various noises and channels, is a major challenge for speaker verification. In this paper, a variational autoencoder (VAE) is designed to learn the patterns of speaker embeddings extracted from noisy speech segments, including i-vector and xvector, and generate embeddings with more diversity to improve the robustness of speaker verification systems with probabilistic linear discriminant analysis (PLDA) back-end. The approach is evaluated on the standard NIST SRE 2016 dataset. Compared to manual and generative adversarial network (GAN) based augmentation approaches, the proposed VAE based augmentation achieves a slightly better performance for i-vector on Tagalog and Cantonese with EERs of 15.54\% and 7.84\%, and a more significant improvement for x-vector on those two languages with EERs of 11.86\% and 4.20\%.}
}

@inproceedings{Han:2019rc,
  title = {On-Device Image Classification with Proxyless Neural Architecture Search and Quantization-Aware Fine-Tuning},
  author = {Cai, Han and Wang, Tianzhe and Wu, Zhanghao and Wang, Kuan and Lin, Ji and Han, Song},
  booktitle = {ICCV workshop},
  year = {2019},
  slides = {Presentation_on_device.pdf},
  url = {http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Cai_On-Device_Image_Classification_with_Proxyless_Neural_Architecture_Search_and_Quantization-Aware_ICCVW_2019_paper.html},
  pdf = {http://openaccess.thecvf.com/content_ICCVW_2019/papers/LPCV/Cai_On-Device_Image_Classification_with_Proxyless_Neural_Architecture_Search_and_Quantization-Aware_ICCVW_2019_paper.pdf},
  image = {on_device.png},
  abstract = {It is challenging to efficiently deploy deep learning models on resource-constrained hardware devices (e.g., mobile and IoT devices) with strict efficiency constraints (e.g., latency, energy consumption). We employ Proxyless Neural Architecture Search (ProxylessNAS) to auto design compact and specialized neural network architectures for the target hardware platform. ProxylessNAS makes latency differentiable, so we can optimize not only accuracy but also latency by gradient descent. Such direct optimization saves the search cost by 200x compared to conventional neural architecture search methods. Our work is followed by quantization-aware fine-tuning to further boost efficiency. In the Low Power Image Recognition Competition at CVPR'19, our solution won the 3rd place on the task of Real-Time Image Classification (online track).}
}
