---
---


@inproceedings{Wu:2020efficient,
  title = {Lite Transformer with Long-Short Range Attention},
  author = {Wu*, Zhanghao and Liu*, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
  booktitle = {ICLR},
  year = {2020},
  url = {https://openreview.net/forum?id=ByeMPlHKPH},
  pdf = {https://arxiv.org/pdf/2004.11886v1.pdf},
  image = {enmt.png},
  slides = {Presentation_LiteTransformer.pdf},
  website = {https://zhanghaowu.me/pubs/LiteTransformer/index.html},
  abstract = {Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at <a href="https://github.com/mit-han-lab/lite-transformer">https://github.com/mit-han-lab/lite-transformer</a>.}
}

@article{Wu:2020hat,
  title = {HAT: Hardware-Aware Transformers for Efficient Natural Language Processing},
  author = {Wang, Hanrui and Wu, Zhanghao and Liu, Zhijian and Cai, Han and Zhu, Ligeng and Han, Song},
  journal = {To appear in ACL},
  year = {2020},
  image = {hat.png},
  abstract = {Transformers are ubiquitous in natural language processing but hard to deploy due to the intensive computation. To enable low latency inference on different hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we efficiently train all candidates (SubTransformers) in the design space with a weight-shared SuperTransformer. Finally, We conduct an evolutionary search with hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the targeted hardware. Extensive experiments on three machine translation tasks demonstrate that our framework can discover efficient Transformers for different hardware (CPU, GPU, IoT device). Remarkably, when running WMT'14 translation task on Raspberry Pi 4 ARM CPU, HAT can achieve 3x speedup, 3.7x smaller size over baseline Transformer; 2.7x speedup, 3.6x smaller size over Evolved Transformer with 12,041x less search cost and no performance loss.}
}

@article{Wu:2020FaceMix,
    title = {FaceMix: Privacy-preserving Facial Attribute Classification on the Cloud},
    author = {Liu*, Zhijian and Wu*, Zhanghao and Zhu, Ligeng and Gan, Chuang and Han, Song},
    journal = {Submitted to ECCV},
    year = {2020},
    slides = {Presentation_Privacy-Preserving_Edge-Cloud_Inference.pdf},
    demo = {https://www.youtube.com/watch?v=nLVWIl-h5Zc&feature=youtu.be},
    image = {privacy_cvpr.png},
    abstract = {Deep neural networks are widely deployed on edge devices (e.g., for facial attribute classification). Users either perform the inference locally (i.e., edge-based) or send the data to the cloud and run the inference remotely (i.e., cloud-based). However, both solutions have their limitations: edge devices are heavily constrained by insufficient hardware resources and cannot afford to run large models; cloud servers, if not trustworthy, will raise serious privacy issues. In this paper, we mediate between the resource-constrained edge devices and the privacy-invasive cloud servers by introducing a novel privacy-preserving edge-cloud inference framework, FaceMix, for the facial attribute classification networks. We offload the majority of the computations to the cloud and leverage a pair of encryption and decryption functions to protect the privacy of the data transmitted to the cloud. Our framework has three advantages. First, it is privacy-preserving as our encryption cannot be inverted without user's private key. Second, our framework is accuracy-preserving because our encryption takes advantage of the linearity, and we train the model in an encryption-aware manner to help maintain the accuracy. Third, our solution is efficient on the edge since the majority of the workload is delegated to the cloud, and our encryption and decryption processes introduce very few extra computations. Also, our framework introduces small communication overhead and maintains high hardware utilization on the cloud. Extensive experiments on multiple facial attribute classification datasets demonstrate that our framework can greatly reduce the local computations on the edge (to fewer than 20% of FLOPs) with negligible loss of accuracy and no leakages of private information.}
}

@inproceedings{Wu:2019vaeda, 
  title = {Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification}, 
  author = {Wu, Zhanghao and Wang, Shuai and Qian, Yanmin and Yu, Kai}, 
  booktitle = {Interspeech}, 
  doi = {10.21437/Interspeech.2019-2248},
  year = {2019},
  url = {http://dx.doi.org/10.21437/Interspeech.2019-2248},
  pdf = {https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2248.pdf},
  oral = {},
  slides = {Presentations_VAE_DA.pdf},
  image = {vae_da.png},
  abstract = {Domain or environment mismatch between training and testing, such as various noises and channels, is a major challenge for speaker verification. In this paper, a variational autoencoder (VAE) is designed to learn the patterns of speaker embeddings extracted from noisy speech segments, including i-vector and xvector, and generate embeddings with more diversity to improve the robustness of speaker verification systems with probabilistic linear discriminant analysis (PLDA) back-end. The approach is evaluated on the standard NIST SRE 2016 dataset. Compared to manual and generative adversarial network (GAN) based augmentation approaches, the proposed VAE based augmentation achieves a slightly better performance for i-vector on Tagalog and Cantonese with EERs of 15.54\% and 7.84\%, and a more significant improvement for x-vector on those two languages with EERs of 11.86\% and 4.20\%.}
}

@inproceedings{Han:2019rc,
  title = {On-Device Image Classification with Proxyless Neural Architecture Search and Quantization-Aware Fine-Tuning},
  author = {Cai, Han and Wang, Tianzhe and Wu, Zhanghao and Wang, Kuan and Lin, Ji and Han, Song},
  booktitle = {ICCV workshop},
  year = {2019},
  slides = {Presentation_on_device.pdf},
  url = {http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Cai_On-Device_Image_Classification_with_Proxyless_Neural_Architecture_Search_and_Quantization-Aware_ICCVW_2019_paper.html},
  pdf = {http://openaccess.thecvf.com/content_ICCVW_2019/papers/LPCV/Cai_On-Device_Image_Classification_with_Proxyless_Neural_Architecture_Search_and_Quantization-Aware_ICCVW_2019_paper.pdf},
  image = {on_device.png},
  abstract = {It is challenging to efficiently deploy deep learning models on resource-constrained hardware devices (e.g., mobile and IoT devices) with strict efficiency constraints (e.g., latency, energy consumption). We employ Proxyless Neural Architecture Search (ProxylessNAS) to auto design compact and specialized neural network architectures for the target hardware platform. ProxylessNAS makes latency differentiable, so we can optimize not only accuracy but also latency by gradient descent. Such direct optimization saves the search cost by 200x compared to conventional neural architecture search methods. Our work is followed by quantization-aware fine-tuning to further boost efficiency. In the Low Power Image Recognition Competition at CVPR'19, our solution won the 3rd place on the task of Real-Time Image Classification (online track).}
}
