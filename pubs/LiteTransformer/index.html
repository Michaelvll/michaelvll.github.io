<!doctype html>
<html>

<head>
 <title>Lite Transformer</title>
 <meta name="viewport" content="width=device-width,initial-scale=1">
 <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
  integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
 <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
 <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
 <link href="style.css" rel="stylesheet">

 <style>
  .paperthumb {
   float: left;
   width: 120px;
   margin: 3px 10px 7px 0;
  }

  .paperdesc {
   clear: both;
  }
 </style>
</head>

<body class="nd-docs" data-gr-c-s-loaded="true">
 <div class="nd-pageheader">
  <div class="container">
   <p class="lead" style="font-size:200%">
    <a href="https://openreview.net/pdf?id=ByeMPlHKPH">Lite Transformer with Long-Short Range Attention</a>
    <address>
     <nobr><a href="https://zhanghaowu.me">Zhanghao Wu</a><sup>1,2,*</sup>,</nobr>
     <nobr>Zhijian Liu<sup>1,*</sup>,</nobr>
     <nobr>Ji Lin<sup>1</sup>,</nobr>
     <nobr>Yujun Lin<sup>1</sup>,</nobr>
     <nobr><a href="https://songhan.mit.edu/">Song Han</a><sup>1</sup>,</nobr>
     <br>
     <nobr><sup>1</sup>Massachusetts Institute of Technology,</nobr>
     <nobr><sup>2</sup>Shanghai Jiao Tong University</nobr>
    </address>
   </p>
  </div>
 </div> <!-- end nd-pageheader -->

 <div class="container">

  <div class="row">
   <div class="col text-center">
    <p>
     <a href="https://arxiv.org/pdf/2004.11886.pdf"
      class="d-inline-block p-3"><img height="100" width="78" src="figures/paper-thumbnail.png" style="border:1px solid"
       data-nothumb><br>ICLR 2020 Paper</a>
     <a href="https://zhanghaowu.me/assets/pdf/Presentation_LiteTransformer.pdf" class="d-inline-block p-3"><img src="figures/slides-thumbnail.png"
       style="border:1px solid; margin-bottom:16px" height="75" data-nothumb><br>Slides</a>
    <a href="https://github.com/mit-han-lab/lite-transformer" class="d-inline-block p-3"><img height="100" width="78" src="figures/code-thumbnail.png" style="border:1px solid" data-nothumb><br>Code</a>
   </div>
  </div>

  <div class="row">
   <div class="col">

    <p style="text-align: justify;text-justify: inter-word">Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the <strong>local</strong> context modeling (by convolution) while another group specializes in the <strong>long-distance</strong> relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by <strong>18.2x</strong>. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU <strong>years</strong>. Code has been made available at <a href="https://github.com/mit-han-lab/lite-transformer">https://github.com/mit-han-lab/lite-transformer</a>.</p>

    <h2>Lite Transformer</h2>

    <p>Our paper presents a Lite Transformer with Long-Short Range Attention (LSRA):</p>
    <p style="text-align: center"><img src="figures/overview.png" style="max-width:90%"></p>
    <ol>
     <li>The attention branch can specialize in global feature extraction.</li>
     <li>The local feature extraction is sepcialized by a convolutional branch which efficiently models locality (diagonal structure).</li>
    </ol>

    <h2>Consistent Improvement on Machine Translation and Language Modeling</h2>
        <p style="text-align: center"><img src="figures/tradeoff.png" style="max-width:90%"></p>
    <h2>Save 20000x Searching Cost of Evolved Transformer</h2>
        <p style="text-align: right"><img src="figures/et.png" style="max-width:95%"></p>
    <div class="row">
        <div class="column">
          <h2>Less CO<sub>2</sub> Emission</h2>
          <p style="text-align: center"><img src="figures/results-co2.png" style="max-width:90%"></p>
          </div>
        <div class="column">
          <h2>18.2x Compression</h2>
            <p style="text-align: center"><img src="figures/compression.png" style="max-width:95%"></p>
        </div>
    </div>
    
    <!--
    <h2>Introduction Video</h2>
    <center class="embed-responsive embed-responsive-16by9">
    <iframe class="embed-responsive-item" width="560" height="315" src="????" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </center>
    -->

    <h2>Citation</h2>

    <pre class="highlight">
@inproceedings{Wu2020LiteTransformer,
  title={Lite Transformer with Long-Short Range Attention},
  author={Zhanghao Wu* and Zhijian Liu* and Ji Lin and Yujun Lin and Song Han},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}
</pre>

    <p style="text-align: justify;text-justify: inter-word"><strong>Acknowledgments</strong>: We sincerely thank MIT-IBM Watson AI Lab, Facebook Faculty Award, Google-Daydream Research Award, and AWS Machine Learning Research Award for supporting this research.</p>

   </div>
  </div> <!-- row -->

 </div> <!-- container -->

</body>

</html>
