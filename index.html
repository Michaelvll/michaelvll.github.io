<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Zhanghao Wu</title>
  <meta name="description" content="Zhanghao Wu's personal website
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/">
</head>

<!-- Mathjax Support -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">About</a>

        <!-- Blog -->
        

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/projects/">Projects</a>
          
        
          
            <a class="page-link" href="/publications/">Publications</a>
          
        
          
        

        <!-- CV link -->
        <a class="page-link" href="/assets/pdf/Zhanghao_Wu_CV.pdf">CV</a>

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title"><black>Zhanghao <slim>Wu</slim></black></h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content <black>Zhanghao <slim>Wu</slim></black> clearfix">
    
  <div class="profile col one right">
    
      <img class="one" src="/assets/img/my_pic.jpg">
    
    

    
    <div class="address">
        <center>
        
            
            <a href="mailto:zhanghao.wu@outlook.com">Email</a>
        
        
        
             / 
            
            <a href="https://www.github.com/Michaelvll" target="_blank" title="GitHub">GitHub</a>
        
        
        </center>
    </div>
    
  </div>


<p>I am a senior student majoring in <strong>computer science</strong> at <a href="http://en.sjtu.edu.cn">Shanghai Jiao Tong University</a>. I am a member of the ACM Honors Class, which is an elite CS program for the top 5% talented students.</p>

<p><strong>My overall GPA is 92.47/100, Rank: 2/37</strong>.</p>

<p>Currently, I am a <strong>visiting student</strong> at MIT, working with Prof. <a href="https://songhan.mit.edu">Song Han</a>.</p>

<p>I am interested in <strong>deep learning</strong>, especially <strong>natural language processing</strong>, <strong>speech</strong>, and <strong>efficient machine learning</strong>.</p>

<p>I plan to pursue a CS Ph.D. that starts in the fall, 2020.</p>

<h3 id="publications">Publications</h3>
<ol class="bibliography"><li>

<div id="Wu:2020efficient">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/enmt.png">
            <img src="/assets/papers/img/enmt.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>Efficient Transformer for Mobile Applications</strong></span>
            <span class="author">
            
                
                
                    
                    <em>Zhanghao Wu</em>,
                    
                
                
            
                
                
                    
                    
                        Zhijian Liu,
                    
                    
                
                
            
                
                
                    
                    
                        Ji Lin,
                    
                    
                
                
            
                
                
                    
                    
                        Yujun Lin,
                    
                    
                
                
            
                
                
                    
                    
                        and Song Han
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>Submitted to ICLR</em>
            
            
            2020
            
            
            </span>
        

        <span class="links">
        
            [<a class="abstract">Abstract</a>]
        
        
        
        
            [<a href="https://openreview.net/pdf?id=ByeMPlHKPH" target="_blank">PDF</a>]
        
        
        
        
        
            [<a href="/assets/pdf/Presentation_ENMT.pdf" target="_blank">Slides</a>]
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for real-world mobile applications since mobile phones are tightly constrained by the hardware resources and battery. In this paper, we investigate the mobile setting (under 500M Mult-Adds) for NLP tasks to facilitate the deployment on the edge devices. We present Long-Short Range Attention (LSRA), where some heads specialize in the local context modeling (by convolution) while the others capture the long-distance relationship (by attention). Based on this primitive, we design Mobile Transformer (MBT) that is tailored for the mobile NLP application. Our MBT demonstrates consistent improvement over the transformer on two well-established language tasks: IWSLT 2014 German-English and WMT 2014 English-German. It outperforms the transformer by 0.9 BLEU under 500M Mult-Adds and 1.1 BLEU under 100M Mult-Adds on WMT’14 English-German. Without the costly architecture search that requires more than 250 GPU years, our manually-designed MBT achieves 0.4 higher BLEU than the AutoML-based Evolved Transformer under the extremely efficient mobile setting (i.e., 100M Mult-Adds).</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2020hat">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/hat.png">
            <img src="/assets/papers/img/hat.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>HAT: Hardware-Aware Transformers for Efficient Neural Machine Translation</strong></span>
            <span class="author">
            
                
                
                    
                    <em>Zhanghao Wu*</em>,
                    
                
                
            
                
                
                    
                    
                        Hanrui Wang*,
                    
                    
                
                
            
                
                
                    
                    
                        Zhijian Liu*,
                    
                    
                
                
            
                
                
                    
                    
                        Han Cai,
                    
                    
                
                
            
                
                
                    
                    
                        Ligeng Zhu,
                    
                    
                
                
            
                
                
                    
                    
                        and Song Han
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>Submitted to ACL</em>
            
            
            2020
            
            
            </span>
        

        <span class="links">
        
            [<a class="abstract">Abstract</a>]
        
        
        
        
        
        
        
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Transformers are ubiquitous in natural language processing but hard to deploy due to the intensive computation. To enable low latency inference on different hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we efficiently train all candidates (SubTransformers) in the design space with a weight-shared SuperTransformer. Finally, We conduct an evolutionary search with hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the targeted hardware. Extensive experiments on three machine translation tasks demonstrate that our framework can discover efficient Transformers for different hardware (CPU, GPU, IoT device). Remarkably, when running WMT’14 translation task on Raspberry Pi 4 ARM CPU, HAT can achieve 3x speedup, 3.7x smaller size over baseline Transformer; 2.7x speedup, 3.6x smaller size over Evolved Transformer with 12,041x less search cost and no performance loss.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2020FaceMix">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/privacy_cvpr.png">
            <img src="/assets/papers/img/privacy_cvpr.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>FaceMix: Privacy-preserving Facial Attribute Classification on the Cloud</strong></span>
            <span class="author">
            
                
                
                    
                    <em>Zhanghao Wu*</em>,
                    
                
                
            
                
                
                    
                    
                        Zhijian Liu*,
                    
                    
                
                
            
                
                
                    
                    
                        Ligeng Zhu,
                    
                    
                
                
            
                
                
                    
                    
                        Chuang Gan,
                    
                    
                
                
            
                
                
                    
                    
                        and Song Han
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>Submitted to CVPR</em>
            
            
            2020
            
            
            </span>
        

        <span class="links">
        
            [<a class="abstract">Abstract</a>]
        
        
        
        
        
        
        
        
            [<a href="/assets/pdf/Privacy-Preserving_Edge-Cloud_Inference" target="_blank">Slides</a>]
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Deep neural networks are widely deployed on edge devices (e.g., for facial attribute classification). Users either perform the inference locally (i.e., edge-based) or send the data to the cloud and run the inference remotely (i.e., cloud-based). However, both solutions have their limitations: edge devices are heavily constrained by insufficient hardware resources and cannot afford to run large models; cloud servers, if not trustworthy, will raise serious privacy issues. In this paper, we mediate between the resource-constrained edge devices and the privacy-invasive cloud servers by introducing a novel privacy-preserving edge-cloud inference framework, FaceMix, for the facial attribute classification networks. We offload the majority of the computations to the cloud and leverage a pair of encryption and decryption functions to protect the privacy of the data transmitted to the cloud. Our framework has three advantages. First, it is privacy-preserving as our encryption cannot be inverted without user’s private key. Second, our framework is accuracy-preserving because our encryption takes advantage of the linearity, and we train the model in an encryption-aware manner to help maintain the accuracy. Third, our solution is efficient on the edge since the majority of the workload is delegated to the cloud, and our encryption and decryption processes introduce very few extra computations. Also, our framework introduces small communication overhead and maintains high hardware utilization on the cloud. Extensive experiments on multiple facial attribute classification datasets demonstrate that our framework can greatly reduce the local computations on the edge (to fewer than 20% of FLOPs) with negligible loss of accuracy and no leakages of private information.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2019vaeda">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/vae_da.png">
            <img src="/assets/papers/img/vae_da.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification</strong></span>
            <span class="author">
            
                
                
                    
                    <em>Zhanghao Wu</em>,
                    
                
                
            
                
                
                    
                    
                        Shuai Wang,
                    
                    
                
                
            
                
                
                    
                    
                        Yanmin Qian,
                    
                    
                
                
            
                
                
                    
                    
                        and Kai Yu
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In Interspeech</em>
            
            
            2019
            
            
                <strong style="color: #A31F34"> (Oral)</strong>
            
            </span>
        

        <span class="links">
        
            [<a class="abstract">Abstract</a>]
        
        
        
        
            [<a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2248.pdf" target="_blank">PDF</a>]
        
        
        
        
        
            [<a href="/assets/pdf/Presentations_VAE_DA.pdf" target="_blank">Slides</a>]
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Domain or environment mismatch between training and testing, such as various noises and channels, is a major challenge for speaker verification. In this paper, a variational autoencoder (VAE) is designed to learn the patterns of speaker embeddings extracted from noisy speech segments, including i-vector and xvector, and generate embeddings with more diversity to improve the robustness of speaker verification systems with probabilistic linear discriminant analysis (PLDA) back-end. The approach is evaluated on the standard NIST SRE 2016 dataset. Compared to manual and generative adversarial network (GAN) based augmentation approaches, the proposed VAE based augmentation achieves a slightly better performance for i-vector on Tagalog and Cantonese with EERs of 15.54% and 7.84%, and a more significant improvement for x-vector on those two languages with EERs of 11.86% and 4.20%.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Han:2019rc">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/on_device.png">
            <img src="/assets/papers/img/on_device.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>On-Device Image Classification with Proxyless Neural Architecture Search and Quantization-Aware Fine-Tuning</strong></span>
            <span class="author">
            
                
                
                    
                    
                        Han Cai,
                    
                    
                
                
            
                
                
                    
                    
                        Tianzhe Wang,
                    
                    
                
                
            
                
                
                    
                    <em>Zhanghao Wu</em>,
                    
                
                
            
                
                
                    
                    
                        Kuan Wang,
                    
                    
                
                
            
                
                
                    
                    
                        Ji Lin,
                    
                    
                
                
            
                
                
                    
                    
                        and Song Han
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In ICCV workshop</em>
            
            
            2019
            
            
            </span>
        

        <span class="links">
        
            [<a class="abstract">Abstract</a>]
        
        
        
        
            [<a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/LPCV/Cai_On-Device_Image_Classification_with_Proxyless_Neural_Architecture_Search_and_Quantization-Aware_ICCVW_2019_paper.pdf" target="_blank">PDF</a>]
        
        
        
        
        
            [<a href="/assets/pdf/Presentation_on_device.pdf" target="_blank">Slides</a>]
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>It is challenging to efficiently deploy deep learning models on resource-constrained hardware devices (e.g., mobile and IoT devices) with strict efficiency constraints (e.g., latency, energy consumption). We employ Proxyless Neural Architecture Search (ProxylessNAS) to auto design compact and specialized neural network architectures for the target hardware platform. ProxylessNAS makes latency differentiable, so we can optimize not only accuracy but also latency by gradient descent. Such direct optimization saves the search cost by 200x compared to conventional neural architecture search methods. Our work is followed by quantization-aware fine-tuning to further boost efficiency. In the Low Power Image Recognition Competition at CVPR’19, our solution won the 3rd place on the task of Real-Time Image Classification (online track).</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li></ol>

<h3 id="education">Education</h3>

<p><img src="assets/img/mit.png" width="64" height="33.2" style="float:left; margin:18px 8px 0px 0px" /></p>
<h4 id="hanlab-mit"><a href="https://songhan.mit.edu"><strong>HanLab</strong></a>, MIT</h4>

<p>Research assistant, working with Prof. <a href="https://songhan.mit.edu">Song Han</a> at HanLab, MIT, from Jul. 2019 to present.</p>
<ul>
  <li>Worked on privacy-preserving and efficient machine learning. Designed a privacy-preserving cloud-edge inference method utilizing the linearity of neural networks. Wrote a paper and submitted it to <strong>CVPR</strong> 2019.</li>
  <li>Devoted to efficient natural language processing, especially for machine translation. Proposed a novel primitive with higher capacity than the original transformer under mobile settings. Wrote a paper and submitted it to <strong>ICLR</strong> 2020. The paper just received very positive reviews (Average score: <strong>6.7/8</strong>).</li>
  <li>Focused on the hardware-aware transformer. Automatically designed neural architectures specialized for different hardware with optimal tradeoff between latency and BLEU score. Prepared a paper for <strong>ACL</strong> 2020.</li>
  <li>Won <strong>1st</strong> place in the CVPR’19 Visual Wake Words Challenge and <strong>3rd place</strong> (<strong>1st</strong> place of all academic groups) in CVPR’19 Low Power Image Recognition Challenge.</li>
</ul>

<p><img src="assets/img/sjtu.png" width="64" height="64" style="float:left; margin:5px 8px 0px 0px" /></p>
<h4 id="speechlab-sjtu"><a href="https://speechlab.sjtu.edu.cn/"><strong>SpeechLab</strong></a>, SJTU</h4>

<p>Undergraduate researcher, advised by Prof. <a href="https://speechlab.sjtu.edu.cn/members/yanmin_qian">Yanmin Qian</a> and Prof. <a href="https://speechlab.sjtu.edu.cn/members/kai_yu">Kai Yu</a> at SpeechLab, SJTU, from Jul. 2018 to Jun. 2019.</p>
<ul>
  <li>Focused on Rich Audio Analysis (RAA), analysis, and classification of non-text information within speech.</li>
  <li>Implemented Deep Canonical Correlation Analysis (DCCA) in PyTorch and released the code on GitHub.</li>
  <li>Deep Learning Book Translation: Reinforcement Learning: An Introduction by Sutton, R.S., Barto, A.G.</li>
  <li>Established a VAE based data augmentation to improve the robustness of speaker verification systems by modeling the patterns of noise and reverberation in the speaker embeddings. It was accepted by <strong>Interspeech</strong> 2019 <strong>(oral)</strong>.</li>
</ul>

<h3 id="honors--award">Honors &amp; Award</h3>
<ul>
  <li><strong>1st place</strong>, in Visual Wake Words (VWW) Challenge of CVPR’19, 2019.</li>
  <li><strong>3rd place</strong>, in Low Power Image Recognition Challenge of CVPR’19 (1st place of academic groups), 2019.</li>
  <li><strong>Outstanding Winner</strong>,in Mathematical Contest in Modeling (top 0.5%), 2017.</li>
  <li><strong>Chinese National Scholarship</strong>, highest honor for undergraduates, top 0.2% nation wide, 2018 &amp; 2019.</li>
  <li><strong>Fan Hsu-Chi Chancellor’s Scholarship</strong>, top 0.1%of 17,000 students in SJTU, 2017.</li>
  <li><strong>Zhiyuan Honorary Scholarship</strong>, top 5%of 17,000 students in SJTU, 2016-2018.</li>
</ul>



  </article>

  

  

</div>

      </div>
    </div>

    
        <div class="wrapper">
<center>
    &copy; Copyright 2019 Zhanghao Wu.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>.

    
</center>
</div>


    

    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-125868731-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
