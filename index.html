<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Zhanghao Wu</title>
  <meta name="description" content="Zhanghao Wu's personal website
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/">
</head>




  <body>

    <header class="site-header">

  <div class="wrapper">

    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">About</a>

        <!-- Blog -->
        

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/publications/">Publications</a>
          
        

        <!-- CV link -->
        

        <!-- Valentine's Day -->
        <a class="page-link" style="color:red" href="/assets/html/valentine.html">&nbsp;&nbsp;&nbsp;&nbsp;</a>

      </div>
    </nav>

  </div>
  <!-- Place this tag in your head or just before your close body tag. -->
  <script async defer src="https://buttons.github.io/buttons.js"></script>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Zhanghao Wu (吴章昊)</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Zhanghao Wu (吴章昊) clearfix">
    
  <div class="profile col one right">
    
      <a href="/assets/img/Sky-headshot-ps.JPG">
        <img class="one" src="/assets/img/Sky-headshot-ps.JPG">
      </a>
    
    

    <div class="address">
        <center>
        
            
            
            zhwu[@]berkeley[.]edu
            <p></p>
        
        
            
            
            <a href="https://scholar.google.com/citations?user=YfyMDFgAAAAJ&hl"  target="_blank" title="Google Scholar">Scholar</a>
        
        
             | 
            
            <a href="https://www.github.com/Michaelvll" target="_blank" title="GitHub">GitHub</a>
        
        
        </center>
    </div>
    
  </div>


<div class="content">
<p>I fortunately joined the CS Ph.D. program at <a href="https://sky.cs.berkeley.edu/">UC Berkeley Sky Computing Lab</a> in the fall, 2020. I am currently working with Prof. <a href="http://people.eecs.berkeley.edu/~istoica/">Ion Stoica</a> on Sky Computing over clouds, specially for AI.</p>

<p>I am focusing on building <a href="https://skypilot.readthedocs.io/en/latest/">SkyPilot</a>, a framework for easily and cost effectively running ML and batch jobs on any cloud, which aims to realize the <a href="https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s02-stoica.pdf">Sky Computing vision</a>. Please check out our system on <a href="https://github.com/skypilot-org/skypilot">Github</a>. The paper is available in NSDI’23 and the latest paper for broker policy “Can’t Be Late” will be available in NSDI’24.</p>

<p>Before coming to Berkeley, I was an undergraduate student majoring in computer science at Shanghai Jiao Tong University (<a href="http://en.sjtu.edu.cn/">SJTU</a>), a member of the SJTU ACM Honors Class, and a research intern working with <a href="https://speechlab.sjtu.edu.cn/members/kai_yu">Prof. Kai Yu</a> and <a href="https://speechlab.sjtu.edu.cn/members/yanmin_qian">Prof. Yanmin Qian</a> at SJTU SpeechLab. I also had a wonderful time as a research assistant working with <a href="https://songhan.mit.edu">Prof. Song Han</a> at MIT HAN Lab.</p>

<!-- My research interests lie in **efficient deep learning**, especially for **natural language processing** and **speech**, and **system**. -->

<h3 id="news">News</h3>

<ul>
  <li><strong>[2024.04]</strong> Our “Can’t Be Late” <a href="https://www.usenix.org/system/files/nsdi24-wu-zhanghao.pdf">paper</a> is appearing in NSDI’24 and won the <strong>Outstanding Paper Award</strong>.</li>
  <li><strong>[2024.01]</strong> <a href="https://research.ibm.com/university/awards/fellowships-awardees.html">IBM Fellowship, 2023</a>.</li>
  <li><strong>[2023.04]</strong> Our SkyPilot <a href="https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng">paper</a> is appearing in NSDI’23.
<!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/skypilot-org/skypilot" data-show-count="true" aria-label="Star skypilot-org/skypilot on GitHub">Star</a></li>
  <li><strong>[2023.03]</strong> An open-source chatbot, <a href="https://vicuna.lmsys.org">Vicuna</a>, powered by SkyPilot is released with a <a href="https://chat.lmsys.org">demo</a>. 
<!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/lm-sys/FastChat" data-show-count="true" aria-label="Star lm-sys/FastChat on GitHub">Star</a></li>
</ul>

<h3 id="publications">Publications</h3>
<ol class="bibliography"><li>

<div id="Wu:2024SkySpot">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/skyspot.png">
            <img src="/assets/papers/img/skyspot.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>Can’t Be Late: Optimizing Spot Instance Savings under Deadlines</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    <em><a href="">Zhanghao Wu</a></em>,
                    
                
                
            
                
                
                    
                    
                        Wei-Lin Chiang,
                    
                    
                
                
            
                
                
                    
                    
                        Ziming Mao,
                    
                    
                
                
            
                
                
                    
                    
                        Zongheng Yang,
                    
                    
                
                
            
                
                
                    
                    
                        Eric Friedman,
                    
                    
                
                
            
                
                
                    
                    
                        Scott Shenker,
                    
                    
                
                
            
                
                
                    
                    
                        and Ion Stoica
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In NSDI (Outstanding Paper Award)</em>
            
            
            2024
            
            
                <strong> (Outstanding Paper Award)</strong>
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://www.usenix.org/system/files/nsdi24-wu-zhanghao.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
        
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Cloud providers offer spot instances alongside on-demand instances to optimize resource utilization. While economically appealing, spot instances’ preemptible nature causes them ill-suited for deadline-sensitive jobs. To allow jobs to meet deadlines while leveraging spot instances, we propose a simple idea: use on-demand instances judiciously as a backup resource. However, due to the unpredictable spot instance availability, determining when to switch between spot and on-demand to minimize cost requires careful policy design. In this paper, we first provide an in-depth characterization of spot instances (e.g., availability, pricing, duration), and develop a basic theoretical model to examine the worst and average-case behaviors of baseline policies (e.g., greedy). The model serves as a foundation to motivate our design of a simple and effective policy, Uniform Progress, which is parameter-free and requires no assumptions on spot availability. Our empirical study, based on three-month-long real spot availability traces on AWS, demonstrates that it can (1) outperform the greedy policy by closing the gap to the optimal policy by 2× in both average and bad cases, and (2) further reduce the gap when limited future knowledge is given. These results hold in a variety of conditions ranging from loose to tight deadlines, low to high spot availability, and on single or multiple instances. By implementing this policy on top of SkyPilot, an intercloud broker system, we achieve 27%-84% cost savings across a variety of representative real-world workloads and deadlines. The spot availability traces are open-sourced for future research.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="zheng2024realchatm">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/lmsys.png">
            <img src="/assets/papers/img/lmsys.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    
                        Lianmin Zheng*,
                    
                    
                
                
            
                
                
                    
                    
                        Wei-Lin Chiang*,
                    
                    
                
                
            
                
                
                    
                    
                        Ying Sheng,
                    
                    
                
                
            
                
                
                    
                    
                        Tianle Li,
                    
                    
                
                
            
                
                
                    
                    
                        Siyuan Zhuang,
                    
                    
                
                
            
                
                
                    
                    <em><a href="">Zhanghao Wu</a></em>,
                    
                
                
            
                
                
                    
                    
                        Yonghao Zhuang,
                    
                    
                
                
            
                
                
                    
                    
                        Zhuohan Li,
                    
                    
                
                
            
                
                
                    
                    
                        Zi Lin,
                    
                    
                
                
            
                
                
                    
                    
                        Eric Xing,
                    
                    
                
                
            
                
                
                    
                    
                        Joseph E. Gonzalez,
                    
                    
                
                
            
                
                
                    
                    
                        Ion Stoica,
                    
                    
                
                
            
                
                
                    
                    
                        and Hao Zhang
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In ICLR</em>
            
            
            2024
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://arxiv.org/pdf/2309.11998.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
        
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset’s content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="zheng2024judging">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/judging.png">
            <img src="/assets/papers/img/judging.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>Judging llm-as-a-judge with mt-bench and chatbot arena</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    
                        Lianmin Zheng*,
                    
                    
                
                
            
                
                
                    
                    
                        Wei-Lin Chiang*,
                    
                    
                
                
            
                
                
                    
                    
                        Ying Sheng*,
                    
                    
                
                
            
                
                
                    
                    
                        Siyuan Zhuang,
                    
                    
                
                
            
                
                
                    
                    <em><a href="">Zhanghao Wu</a></em>,
                    
                
                
            
                
                
                    
                    
                        Yonghao Zhuang,
                    
                    
                
                
            
                
                
                    
                    
                        Zi Lin,
                    
                    
                
                
            
                
                
                    
                    
                        Zhuohan Li,
                    
                    
                
                
            
                
                
                    
                    
                        Dacheng Li,
                    
                    
                
                
            
                
                
                    
                    
                        Eric Xing,
                    
                    
                
                
            
                
                
                    
                    
                        and  others
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In NeurIPS</em>
            
            
            2024
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
        
        
             | 
            
            <a href="https://github.com/lm-sys/FastChat" target="_blank">Code</a>
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences.To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions.We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them.We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans.Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna.The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Yang:2023SkyPilot">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/skypilot.png">
            <img src="/assets/papers/img/skypilot.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>SkyPilot: An Intercloud Broker for Sky Computing</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    
                        Zongheng Yang*,
                    
                    
                
                
            
                
                
                    
                    <em><a href="">Zhanghao Wu*</a></em>,
                    
                
                
            
                
                
                    
                    
                        Michael Luo,
                    
                    
                
                
            
                
                
                    
                    
                        Wei-Lin Chiang,
                    
                    
                
                
            
                
                
                    
                    
                        Romil Bhardwaj,
                    
                    
                
                
            
                
                
                    
                    
                        Woosuk Kwon,
                    
                    
                
                
            
                
                
                    
                    
                        Siyuan Zhuang,
                    
                    
                
                
            
                
                
                    
                    
                        Frank Sifei Luan,
                    
                    
                
                
            
                
                
                    
                    
                        Gautam Mittal,
                    
                    
                
                
            
                
                
                    
                    
                        Scott Shenker,
                    
                    
                
                
            
                
                
                    
                    
                        and Ion Stoica
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In NSDI</em>
            
            
            2023
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://www.usenix.org/system/files/nsdi23-yang-zongheng.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
        
        
             | 
            
            <a href="https://github.com/skypilot-org/skypilot" target="_blank">Code</a>
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>To comply with the increasing number of government regulations about data placement and processing, and to protect themselves against major cloud outages, many users want the ability to easily migrate their workloads between clouds. In this paper we propose doing so not by imposing uniform and comprehensive standards, but by creating a fine-grained two-sided market via an intercloud broker. These brokers will allow users to view the cloud ecosystem not just as a collection of individual and largely incompatible clouds but as a more integrated Sky of Computing. We describe the design and implementation of an intercloud broker, named SkyPilot, evaluate its benefits, and report on its real-world usage.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="liu2023single">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/brain.png">
            <img src="/assets/papers/img/brain.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>Single-cell DNA methylome and 3D multi-omic atlas of the adult mouse brain</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    
                        Hanqing Liu,
                    
                    
                
                
            
                
                
                    
                    
                        Qiurui Zeng,
                    
                    
                
                
            
                
                
                    
                    
                        Jingtian Zhou,
                    
                    
                
                
            
                
                
                    
                    
                        Anna Bartlett,
                    
                    
                
                
            
                
                
                    
                    
                        Bang-An Wang,
                    
                    
                
                
            
                
                
                    
                    
                        Peter Berube,
                    
                    
                
                
            
                
                
                    
                    
                        Wei Tian,
                    
                    
                
                
            
                
                
                    
                    
                        Mia Kenworthy,
                    
                    
                
                
            
                
                
                    
                    
                        Jordan Altshul,
                    
                    
                
                
            
                
                
                    
                    
                        Joseph R Nery,
                    
                    
                
                
            
                
                
                    
                    
                        and  others
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>Nature</em>
            
            
            2023
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://www.nature.com/articles/s41586-023-06805-y.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
        
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Cytosine DNA methylation is essential in brain development and is implicated in various neurological disorders. Understanding DNA methylation diversity across the entire brain in a spatial context is fundamental for a complete molecular atlas of brain cell types and their gene regulatory landscapes. Here we used single-nucleus methylome sequencing (snmC-seq3) and multi-omic sequencing (snm3C-seq)1 technologies to generate 301,626 methylomes and 176,003 chromatin conformation–methylome joint profiles from 117 dissected regions throughout the adult mouse brain. Using iterative clustering and integrating with companion whole-brain transcriptome and chromatin accessibility datasets, we constructed a methylation-based cell taxonomy with 4,673 cell groups and 274 cross-modality-annotated subclasses. We identified 2.6 million differentially methylated regions across the genome that represent potential gene regulation elements. Notably, we observed spatial cytosine methylation patterns on both genes and regulatory elements in cell types within and across brain regions. Brain-wide spatial transcriptomics data validated the association of spatial epigenetic diversity with transcription and improved the anatomical mapping of our epigenetic datasets. Furthermore, chromatin conformation diversities occurred in important neuronal genes and were highly associated with DNA methylation and transcription changes. Brain-wide cell-type comparisons enabled the construction of regulatory networks that incorporate transcription factors, regulatory elements and their potential downstream gene targets. Finally, intragenic DNA methylation and chromatin conformation patterns predicted alternative gene isoform expression observed in a whole-brain SMART-seq2 dataset. Our study establishes a brain-wide, single-cell DNA methylome and 3D multi-omic atlas and provides a valuable resource for comprehending the cellular–spatial and regulatory genome diversity of the mouse brain.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="vicuna2023">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/vicuna.jpeg">
            <img src="/assets/papers/img/vicuna.jpeg" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    
                        Wei-Lin Chiang*,
                    
                    
                
                
            
                
                
                    
                    
                        Zhuohan Li*,
                    
                    
                
                
            
                
                
                    
                    
                        Zi Lin*,
                    
                    
                
                
            
                
                
                    
                    
                        Ying Sheng*,
                    
                    
                
                
            
                
                
                    
                    <em><a href="">Zhanghao Wu*</a></em>,
                    
                
                
            
                
                
                    
                    
                        Hao Zhang*,
                    
                    
                
                
            
                
                
                    
                    
                        Lianmin Zheng*,
                    
                    
                
                
            
                
                
                    
                    
                        Siyuan Zhuang*,
                    
                    
                
                
            
                
                
                    
                    
                        Yonghao Zhuang*,
                    
                    
                
                
            
                
                
                    
                    
                        Joseph E. Gonzalez,
                    
                    
                
                
            
                
                
                    
                    
                        Ion Stoica,
                    
                    
                
                
            
                
                
                    
                    
                        and Eric P. Xing
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            
            2023
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
        
             | 
            
            <a href="https://lmsys.org/blog/2023-03-30-vicuna/" target="_blank">Blog</a>
        
        
        
        
        
        
             | 
            
            <a href="https://chat.lmsys.org/" target="_blank">Demo</a>
        
        
             | 
            
            <a href="https://github.com/lm-sys/FastChat" target="_blank">Code</a>
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%* of cases. The cost of training Vicuna-13B is around $300. The code and weights, along with an online demo, are publicly available for non-commercial use.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2021GraphTrans">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/graphtrans.png">
            <img src="/assets/papers/img/graphtrans.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>Representing Long-Range Context for Graph Neural Networks with Global Attention</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    <em><a href="">Zhanghao Wu*</a></em>,
                    
                
                
            
                
                
                    
                    
                        Paras Jain*,
                    
                    
                
                
            
                
                
                    
                    
                        Matthew Wright,
                    
                    
                
                
            
                
                
                    
                    
                        Azalia Mirhoseini,
                    
                    
                
                
            
                
                
                    
                    
                        Joseph E. Gonzalez,
                    
                    
                
                
            
                
                
                    
                    
                        and Ion Stoica
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In NeurIPS</em>
            
            
            2021
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://proceedings.neurips.cc/paper/2021/file/6e67691b60ed3e4a55935261314dd534-Paper.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
             | 
            
            <a href="/assets/pdf/GraphTrans_Neurips2021_Slides.pdf" target="_blank">Slides</a>
        
        
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel “readout” mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at <a href="https://github.com/ucbrise/graphtrans">https://github.com/ucbrise/graphtrans</a>.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Liang:2021RLlibflow">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/rllibflow.png">
            <img src="/assets/papers/img/rllibflow.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>RLlib Flow: Distributed Reinforcement Learning is a Dataflow Problem</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    
                        Eric Liang*,
                    
                    
                
                
            
                
                
                    
                    <em><a href="">Zhanghao Wu*</a></em>,
                    
                
                
            
                
                
                    
                    
                        Michael Luo,
                    
                    
                
                
            
                
                
                    
                    
                        Sven Mika,
                    
                    
                
                
            
                
                
                    
                    
                        Joseph E. Gonzalez,
                    
                    
                
                
            
                
                
                    
                    
                        and Ion Stoica
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In NeurIPS</em>
            
            
            2021
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://arxiv.org/pdf/2011.12719.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
             | 
            
            <a href="/assets/pdf/RLlib_Flow_Neurips_2021_Slides.pdf" target="_blank">Slides</a>
        
        
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Researchers and practitioners in the field of reinforcement learning (RL) frequently leverage parallel computation, which has led to a plethora of new algorithms and systems in the last few years. In this paper, we re-examine the challenges posed by distributed RL and try to view it through the lens of an old idea: distributed dataflow. We show that viewing RL as a dataflow problem leads to highly composable and performant implementations. We propose RLlib Flow, a hybrid actor-dataflow programming model for distributed RL, and validate its practicality by porting the full suite of algorithms in RLlib, a widely adopted distributed RL library. Concretely, RLlib Flow provides 2-9 code savings in real production code and enables the composition of multi-agent algorithms not possible by end users before. The open-source code is available as part of RLlib at <a href="https://github.com/ray-project/ray/tree/master/rllib">https://github.com/ray-project/ray/tree/master/rllib</a>.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2020DataMix">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/privacy_eccv.png">
            <img src="/assets/papers/img/privacy_eccv.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>DataMix: Efficient Privacy-Preserving Edge-Cloud Inference</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    
                        <a href="http://zhijianliu.com/" target="_blank">Zhijian Liu*</a>, 
                    
                    
                
                
            
                
                
                    
                    <em><a href="">Zhanghao Wu*</a></em>,
                    
                
                
            
                
                
                    
                    
                        <a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        <a href="https://lzhu.me/" target="_blank">Ligeng Zhu</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        and <a href="https://songhan.mit.edu" target="_blank">Song Han</a> 
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In ECCV</em>
            
            
            2020
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560562.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
             | 
            
            <a href="/assets/pdf/Presentation_Privacy-Preserving_Edge-Cloud_Inference.pdf" target="_blank">Slides</a>
        
        
             | 
            
            <a href="https://www.youtube.com/watch?v=nLVWIl-h5Zc&amp;feature=youtu.be" target="_blank">Demo</a>
        
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Deep neural networks are widely deployed on edge devices. Users either perform the inference locally (i.e., edge-based) or send the data to the cloud and run inference remotely (i.e., cloud-based). However, edge devices are heavily constrained by insufficient hardware resources and cannot afford to run large models; cloud servers, if not trustworthy, will raise serious privacy issues. In this paper, we mediate between the resource-constrained edge devices and the privacy-invasive cloud servers by introducing a novel privacy-preserving edge-cloud inference framework, DataMix. We off-load the majority of the computations to the cloud and leverage a pair of mixing and de-mixing operation, inspired by mixup, to protect the privacy of the data transmitted to the cloud. Our framework has three advantages. First, it is privacy-preserving as the mixing cannot be inverted without the user’s private mixing coefficients. Second, our framework is accuracy-preserving because our framework takes advantage of the space spanned by image mixing, and we train the model in a mixing-aware manner to maintain accuracy. Third, our solution is efficient on the edge since the majority of the workload is delegated to the cloud, and mixing and de-mixing introduce few extra computations. DataMix introduces small communication overhead and maintains high hardware utilization on the cloud. Extensive experiments on multiple computer vision and speech recognition datasets demonstrate that our framework can greatly reduce the local computations on the edge (to fewer than 20% of FLOPs) with negligible loss of accuracy and no leakages of private information.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2020hat">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/hat.png">
            <img src="/assets/papers/img/hat.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>HAT: Hardware-Aware Transformers for Efficient Natural Language Processing</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    
                        <a href="https://hanruiwang.me/" target="_blank">Hanrui Wang</a>, 
                    
                    
                
                
            
                
                
                    
                    <em><a href="">Zhanghao Wu</a></em>,
                    
                
                
            
                
                
                    
                    
                        <a href="http://zhijianliu.com/" target="_blank">Zhijian Liu</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        <a href="https://scholar.google.com/citations?user=x-AvvrYAAAAJ&amp;hl=en&amp;authuser=1" target="_blank">Han Cai</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        <a href="https://lzhu.me/" target="_blank">Ligeng Zhu</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        and <a href="https://songhan.mit.edu" target="_blank">Song Han</a> 
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In ACL</em>
            
            
            2020
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://arxiv.org/pdf/2005.14187.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
        
        
        
             | 
            
            <a href="https://hanlab.mit.edu/projects/hat/" target="_blank">Website</a>
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT’14 translation task on Raspberry Pi-4, HAT can achieve 3× speedup, 3.7× smaller size over baseline Transformer; 2.7× speedup, 3.6× smaller size over Evolved Transformer with 12,041× less search cost and no performance loss. HAT code is open-sourced at <a href="https://github.com/mit-hanlab/hardware-aware-transformers.git">https://github.com/mit-hanlab/hardware-aware-transformers.git</a>.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2020efficient">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/enmt.png">
            <img src="/assets/papers/img/enmt.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>Lite Transformer with Long-Short Range Attention</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    <em><a href="">Zhanghao Wu*</a></em>,
                    
                
                
            
                
                
                    
                    
                        <a href="http://zhijianliu.com/" target="_blank">Zhijian Liu*</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        <a href="http://linji.me/" target="_blank">Ji Lin</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        <a href="https://deepai.org/profile/yujun-lin" target="_blank">Yujun Lin</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        and <a href="https://songhan.mit.edu" target="_blank">Song Han</a> 
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In ICLR</em>
            
            
            2020
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://arxiv.org/pdf/2004.11886.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
             | 
            
            <a href="/assets/pdf/Presentation_LiteTransformer.pdf" target="_blank">Slides</a>
        
        
        
        
             | 
            
            <a href="https://hanlab.mit.edu/projects/litetransformer/" target="_blank">Website</a>
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT’14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at <a href="https://github.com/mit-han-lab/lite-transformer">https://github.com/mit-han-lab/lite-transformer</a>.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Han:2019rc">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/on_device.png">
            <img src="/assets/papers/img/on_device.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>On-Device Image Classification with Proxyless Neural Architecture Search and Quantization-Aware Fine-Tuning</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    
                        <a href="https://scholar.google.com/citations?user=x-AvvrYAAAAJ&amp;hl=en&amp;authuser=1" target="_blank">Han Cai</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        <a href="https://sites.google.com/view/tianzhe-wang/home" target="_blank">Tianzhe Wang</a>, 
                    
                    
                
                
            
                
                
                    
                    <em><a href="">Zhanghao Wu</a></em>,
                    
                
                
            
                
                
                    
                    
                        <a href="https://scholar.google.com/citations?user=c1-_-dUAAAAJ&amp;hl=en" target="_blank">Kuan Wang</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        <a href="http://linji.me/" target="_blank">Ji Lin</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        and <a href="https://songhan.mit.edu" target="_blank">Song Han</a> 
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In ICCV workshop</em>
            
            
            2019
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/LPCV/Cai_On-Device_Image_Classification_with_Proxyless_Neural_Architecture_Search_and_Quantization-Aware_ICCVW_2019_paper.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
             | 
            
            <a href="/assets/pdf/Presentation_on_device.pdf" target="_blank">Slides</a>
        
        
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>It is challenging to efficiently deploy deep learning models on resource-constrained hardware devices (e.g., mobile and IoT devices) with strict efficiency constraints (e.g., latency, energy consumption). We employ Proxyless Neural Architecture Search (ProxylessNAS) to auto design compact and specialized neural network architectures for the target hardware platform. ProxylessNAS makes latency differentiable, so we can optimize not only accuracy but also latency by gradient descent. Such direct optimization saves the search cost by 200x compared to conventional neural architecture search methods. Our work is followed by quantization-aware fine-tuning to further boost efficiency. In the Low Power Image Recognition Competition at CVPR’19, our solution won the 3rd place on the task of Real-Time Image Classification (online track).</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li></ol>
<ol class="bibliography"><li>

<div id="Wang:2020da">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/da_taslp.png">
            <img src="/assets/papers/img/da_taslp.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>Data Augmentation Using Deep Generative Models for Embedding Based Speaker Recognition</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    
                        <a href="https://speechlab.sjtu.edu.cn/members/shuai-wang" target="_blank">Shuai Wang</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        Yexin Yang,
                    
                    
                
                
            
                
                
                    
                    <em><a href="">Zhanghao Wu</a></em>,
                    
                
                
            
                
                
                    
                    
                        <a href="https://speechlab.sjtu.edu.cn/members/yanmin_qian" target="_blank">Yanmin Qian</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        and <a href="https://speechlab.sjtu.edu.cn/members/kai_yu" target="_blank">Kai Yu</a> 
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>
            
            
            2020
            
            
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://ieeexplore.ieee.org/abstract/document/9167416" target="_blank">Paper</a>
        
        
        
        
        
        
        
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Data augmentation is an effective method to improve the robustness of embedding based speaker verification systems, which could be applied to either the front-end speaker embedding extractor or the back-end PLDA. Different from the conventional augmentation methods such as manually adding noise or reverberation to the original audios, in this article, we propose to use deep generative models to directly generate more diverse speaker embeddings, which would be used for robust PLDA training. Conditional GAN, and VAE are designed, and investigated for different embedding types, including factor analysis based i-vector, TDNN based x-vector, and ResNet based r-vector. The proposed back-end augmentation methods are evaluated on NIST SRE 2016, and 2018 dataset. Within the popular x-vector, and r-vector framework, the experimental results show that our proposed methods can outperform the traditional audio based back-end augmentation method while different front-end augmentation methods are considered.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2019vaeda">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">
    
        <a href="/assets/papers/img/vae_da.png">
            <img src="/assets/papers/img/vae_da.png" width="100%" />
        </a>
    
    </td>
    <td class="txt-td">
        
            <span class="title"><strong>Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification</strong></span>
            <span class="author nolinkcolor">
            
                
                
                    
                    <em><a href="">Zhanghao Wu</a></em>,
                    
                
                
            
                
                
                    
                    
                        <a href="https://speechlab.sjtu.edu.cn/members/shuai-wang" target="_blank">Shuai Wang</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        <a href="https://speechlab.sjtu.edu.cn/members/yanmin_qian" target="_blank">Yanmin Qian</a>, 
                    
                    
                
                
            
                
                
                    
                    
                        and <a href="https://speechlab.sjtu.edu.cn/members/kai_yu" target="_blank">Kai Yu</a> 
                    
                    
                
                
            
            </span>

            <span class="periodical">
            
            <em>In Interspeech</em>
            
            
            2019
            
            
            
                <strong> (Oral)</strong>
            
            </span>
        

        <span class="links">
        
            
            <a class="abstract">Abstract</a>
        
        
        
        
             | 
            
            <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2248.pdf" target="_blank">Paper</a>
        
        
        
        
        
        
             | 
            
            <a href="/assets/pdf/Presentations_VAE_DA.pdf" target="_blank">Slides</a>
        
        
        
        
        </span>

        <!-- Hidden abstract block -->
        
        <span class="abstract hidden">
            <p>Domain or environment mismatch between training and testing, such as various noises and channels, is a major challenge for speaker verification. In this paper, a variational autoencoder (VAE) is designed to learn the patterns of speaker embeddings extracted from noisy speech segments, including i-vector and xvector, and generate embeddings with more diversity to improve the robustness of speaker verification systems with probabilistic linear discriminant analysis (PLDA) back-end. The approach is evaluated on the standard NIST SRE 2016 dataset. Compared to manual and generative adversarial network (GAN) based augmentation approaches, the proposed VAE based augmentation achieves a slightly better performance for i-vector on Tagalog and Cantonese with EERs of 15.54% and 7.84%, and a more significant improvement for x-vector on those two languages with EERs of 11.86% and 4.20%.</p>
        </span>
        
    </td>
    </tr>
    </table>
</div>
</div>

</li></ol>

<h3 id="education">Education</h3>

<div id="education">
<div class="namecard">
<table>
    
        
    <tr>
    <td class="logo-img-td">
        <a href="https://berkeley.edu"><img src="assets/img/berkeley.png" width="100%" /></a>
    </td>
    <td class="logo-txt-td">
         <a href="https://berkeley.edu">University of California, Berkeley</a>, USA<br />
         Ph.D. student at <a href="https://sky.cs.berkeley.edu/">Sky Computing Lab</a> (aka RISELab, AMPLab). Aug. 2020 - May. 2024.
    </td>
    </tr>
        
    <tr>
    <td class="logo-img-td">
        <a href="https://mit.edu"><img src="assets/img/mit.png" width="100%" /></a>
    </td>
    <td class="logo-txt-td">
         <a href="https://mit.edu">Massachusetts Institute Technology</a>, USA<br />
         Research assistant, working with Prof. <a href="https://songhan.mit.edu">Song Han</a>. Jul. 2019 - Jan. 2020.
    </td>
    </tr>
        
    <tr>
    <td class="logo-img-td">
        <a href="http://en.sjtu.edu.cn"><img src="assets/img/sjtu.png" width="100%" /></a>
    </td>
    <td class="logo-txt-td">
         <a href="http://en.sjtu.edu.cn">Shanghai Jiao Tong University</a>, China<br />
         B.Eng. in Computer Science at ACM Honors Class, advised by <a href="http://www.cs.sjtu.edu.cn/en/PeopleDetail.aspx?id=140">Yong Yu</a>. Sep. 2016 - Jun. 2020.
    </td>
    </tr>
        
</table>
</div>
</div>

<h3 id="honors--award">Honors &amp; Award</h3>
<ul>
  <li><strong>Outstanding Paper Award</strong>, in NSDI’24, 2024.</li>
  <li><strong>IBM Fellowship</strong>, 2024.</li>
  <li><strong>1st place</strong>, in Visual Wake Words (VWW) Challenge of CVPR’19, 2019.</li>
  <li><strong>3rd place</strong>, in Low Power Image Recognition Challenge of CVPR’19 (1st place of academic groups), 2019.</li>
  <li><strong>Outstanding Winner</strong>,in Mathematical Contest in Modeling (top 0.5%), 2017.</li>
  <li><strong>Chinese National Scholarship</strong>, highest honor for undergraduates, top 0.2% nation wide, 2018 &amp; 2019.</li>
  <li><strong>Excellent Graduate Award of SJTU</strong>, the highest honor for graduates at SJTU, 2020.</li>
  <li><strong>Zhiyuan Outstanding Student Scholarship of SJTU</strong>, 16 graduates of SJTU Zhiyuan College, 2020.</li>
  <li><strong>Fan Hsu-Chi Chancellor’s Scholarship</strong>, top 0.1%of 17,000 students in SJTU, 2017.</li>
  <li><strong>Zhiyuan Honorary Scholarship</strong>, top 5% of 17,000 students in SJTU, 2016-2018.</li>
</ul>


</div>

  </article>

  

  

</div>

      </div>
    </div>

    
        <div class="wrapper">
<center>
    &copy; Copyright 2025 Zhanghao Wu.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>.

    
</center>
</div>


    

    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-125868731-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
